{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert input data (Jonathan D. MÃ¼ller)\n",
    "\n",
    "Loads the raw data of branch or soil chambers and splits files into daily files in monthly folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization parameters\n",
    "\n",
    "# Data input\n",
    "project_path =  './'\n",
    "\n",
    "project_path_laser = project_path + '01_rawdata/laser computer/'\n",
    "project_path_tc    = project_path + '01_rawdata/thermocouples/'\n",
    "project_path_irga  = project_path + '01_rawdata/irga/'\n",
    "project_path_par   = project_path + '01_rawdata/PAR/'\n",
    "project_path_flow  = project_path + '01_rawdata/flow/'\n",
    "\n",
    "# Output\n",
    "project_path_output = project_path + '02_preprocessed_data/'\n",
    "\n",
    "# List of months to process\n",
    "# - If empty, all available data is processed\n",
    "# - Otherwise, specify using a string of year-month, e.g. ['2018-06'] or ['2017-03','2018-06']\n",
    "month_list = ['2022-01']\n",
    "\n",
    "# Minimum IRGA pump flow\n",
    "# All gas data from the branch chamber IRGA data below this flow will be removed\n",
    "min_irga_flow = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functions\n",
    "#---------------\n",
    "\n",
    "# Delete all relevant files in a folder.\n",
    "# - Used to remove 1h and 1min monthly files before running to prevent appending to existing files\n",
    "def empty_dir(directory):\n",
    "    files = glob.glob(directory + '*')\n",
    "    for f in files:\n",
    "        month_id = f[-10:-6] + '-' + f[-6:-4]\n",
    "        if(month_id in month_list):\n",
    "            print('Remove ' + f)\n",
    "            os.remove(f)\n",
    "        if(not month_list):\n",
    "            print('Remove ' + f)\n",
    "            os.remove(f)\n",
    "    pass\n",
    "\n",
    "# Read an Aerodyne laser input file, full file\n",
    "def read_laser_file(input_fn):\n",
    "    # Read data\n",
    "    data = [ ]\n",
    "    with open(input_fn) as f:\n",
    "        next(f) # Skip the first line\n",
    "        for line in f:\n",
    "            data.append(re.split(r'\\s', line.strip(), 9))\n",
    "    # build the generator        \n",
    "    for line in data:\n",
    "        if(len(line) < 9):\n",
    "            line.append('')\n",
    "    # first element returned is the columns\n",
    "    columns = ['timestamp','OCS.1','CO2.1','CO2.2','H2O.1','CO2.3','CO.1','OCS.2','CO2.4']\n",
    "    # build the data frame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    #for index, row in df.iterrows():\n",
    "    #    df.loc[df['timestamp'] == row['timestamp'], 'timestamp'] = float(df.loc[df['timestamp'] == row['timestamp'], 'timestamp'])\n",
    "    df['timestamp'] = pd.to_numeric(df['timestamp'])\n",
    "    # Now apply normal conversions\n",
    "    df['timestamp'] = pd.to_datetime(df.timestamp, unit='s', origin=pd.Timestamp('1904-01-01')) - pd.DateOffset(seconds=1) # To convert IGOR-time (i.e. Excel 1904)\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read an Aerodyne laser input file, full file\n",
    "def read_laser_file_by_lines(input_fn, lines):\n",
    "    # Read data\n",
    "    data = [ ]\n",
    "    with open(input_fn) as f:\n",
    "        next(f) # Skip the first line\n",
    "        data.append(re.split(r'\\s', f.readline().strip(), 9))\n",
    "    # build the generator        \n",
    "    for line in data:\n",
    "        if(len(line) < 9):\n",
    "            line.append('')\n",
    "    # first element returned is the columns\n",
    "    columns = ['timestamp','OCS.1','CO2.1','CO2.2','H2O.1','CO2.3','CO.1','OCS.2','CO2.4']\n",
    "    # build the data frame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    #for index, row in df.iterrows():\n",
    "    #    df.loc[df['timestamp'] == row['timestamp'], 'timestamp'] = float(df.loc[df['timestamp'] == row['timestamp'], 'timestamp'])\n",
    "    df['timestamp'] = pd.to_numeric(df['timestamp'])\n",
    "    # Now apply normal conversions\n",
    "    df['timestamp'] = pd.to_datetime(df.timestamp, unit='s', origin=pd.Timestamp('1904-01-01')) - pd.DateOffset(seconds=1) # To convert IGOR-time (i.e. Excel 1904)\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# OLD Read an Aerodyne laser input file, full file\n",
    "def read_laser_file_old(input_fn):\n",
    "    df = pd.read_csv(input_fn, sep=' ', skiprows=[0], index_col=False, header=0, names=['timestamp','OCS.1','CO2.1','CO2.2','H2O.1','CO2.3','CO.1','OCS.2','CO2.4'])\n",
    "    df['timestamp'] = pd.to_datetime(df.timestamp, unit='s', origin=pd.Timestamp('1904-01-01')) - pd.DateOffset(seconds=1) # To convert IGOR-time (i.e. Excel 1904)\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# OLD Read an Aerodyne laser input file, full file\n",
    "def read_laser_file_by_lines_old(input_fn, lines):\n",
    "    df = pd.read_csv(input_fn, sep=' ', skiprows=[0], nrows=lines, index_col=False, header=0, names=['timestamp','OCS.1','CO2.1','CO2.2','H2O.1','CO2.3','CO.1','OCS.2','CO2.4'])\n",
    "    df['timestamp'] = pd.to_datetime(df.timestamp, unit='s', origin=pd.Timestamp('1904-01-01')) - pd.DateOffset(seconds=1) # To convert IGOR-time (i.e. Excel 1904)\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read a CR1000 input file, full file\n",
    "def read_cr1000_file(input_fn):\n",
    "    #df = pd.read_csv(input_fn,skiprows=[0,2,3,4,5], na_values=[\"NAN\"])\n",
    "    df = pd.read_csv(input_fn,skiprows=[0,2,3], na_values=[\"NAN\"])\n",
    "    if(df.columns[0] != 'TIMESTAMP'):\n",
    "        #df = pd.read_csv(input_fn,skiprows=[0,1,3,4,5,6], na_values=[\"NAN\"])\n",
    "        df = pd.read_csv(input_fn,skiprows=[0,1,3,4], na_values=[\"NAN\"])\n",
    "    df.rename(columns={'TIMESTAMP':'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime( df.timestamp, format='%Y-%m-%d %H:%M:%S', utc=True, errors=\"raise\")#errors='coerce')\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read a CR1000 input file by lines, only some initial lines\n",
    "def read_cr1000_file_by_lines(input_fn, lines):\n",
    "    #df = pd.read_csv(input_fn,skiprows=[0,2,3,4], na_values=[\"NAN\"],nrows=lines)\n",
    "    df = pd.read_csv(input_fn,skiprows=[0,2,3], na_values=[\"NAN\"],nrows=lines)\n",
    "    if(df.columns[0] != 'TIMESTAMP'):\n",
    "        #df = pd.read_csv(input_fn,skiprows=[0,1,3,4,5], na_values=[\"NAN\"],nrows=lines)\n",
    "        df = pd.read_csv(input_fn,skiprows=[0,1,3,4], na_values=[\"NAN\"],nrows=lines)\n",
    "    df.rename(columns={'TIMESTAMP':'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime( df.timestamp, format='%Y-%m-%d %H:%M:%S', utc=True, errors=\"raise\")#errors='coerce')\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read a CR1000 input file, full file\n",
    "def read_irga_file(input_fn):\n",
    "    df = pd.read_csv(input_fn,skiprows=[0,2,3,4,5], na_values=[\"NAN\"])\n",
    "    if(df.columns[0] != 'TIMESTAMP'):\n",
    "        df = pd.read_csv(input_fn,skiprows=[0,1,3,4,5,6], na_values=[\"NAN\"])\n",
    "    df.rename(columns={'TIMESTAMP':'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime( df.timestamp, format='%Y-%m-%d %H:%M:%S', utc=True, errors=\"raise\")#errors='coerce')\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read a CR1000 input file by lines\n",
    "def read_irga_file_by_lines(input_fn, lines):\n",
    "    df = pd.read_csv(input_fn,skiprows=[0,2,3,4], na_values=[\"NAN\"],nrows=lines)\n",
    "    if(df.columns[0] != 'TIMESTAMP'):\n",
    "        df = pd.read_csv(input_fn,skiprows=[0,1,3,4,5], na_values=[\"NAN\"],nrows=lines)\n",
    "    df.rename(columns={'TIMESTAMP':'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime( df.timestamp, format='%Y-%m-%d %H:%M:%S', utc=True, errors=\"raise\")#errors='coerce')\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "def remove_obsolete_irga_data(temp):\n",
    "    temp = temp.copy()\n",
    "    \n",
    "    # Rename PAR & temperature\n",
    "    temp.rename(columns={'RadKipZonen':'par.ambient.umol_m2_s1'}, inplace=True)\n",
    "    temp.rename(columns={'Tc(8)':'temp.air.ambient.c'}, inplace=True)\n",
    "    temp.rename(columns={'H2o_6262_mmol_mol':'conc.h2o.irga.ambient.mmol_mol'}, inplace=True)\n",
    "    temp.rename(columns={'Co2_6262_micmol_mol':'conc.co2.irga.ambient.umol_mol'}, inplace=True)\n",
    "    temp.rename(columns={'AirFlow_Amb':'pump.flow.irga.lpm'}, inplace=True)\n",
    "    temp.rename(columns={'Prees_7000':'P.irga.kPa'}, inplace=True)\n",
    "    \n",
    "    # Remove bad data\n",
    "    temp.loc[temp['pump.flow.irga.lpm'] <= min_irga_flow, 'conc.h2o.irga.ambient.mmol_mol'] = np.nan\n",
    "    temp.loc[temp['pump.flow.irga.lpm'] <= min_irga_flow, 'conc.co2.irga.ambient.umol_mol'] = np.nan\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    temp = temp[['timestamp','temp.air.ambient.c','par.ambient.umol_m2_s1','conc.h2o.irga.ambient.mmol_mol','conc.co2.irga.ambient.umol_mol','P.irga.kPa','dayid']]\n",
    "    \n",
    "    return(temp)\n",
    "\n",
    "def fix_tc_names(temp):\n",
    "    temp = temp.copy()\n",
    "    temp.columns = [c.replace('TcRafat_Avg(', 'Tc(') for c in list(temp.columns)]\n",
    "    temp.columns = [c.replace('TcRafat(', 'Tc(') for c in list(temp.columns)]\n",
    "    # Drop record column\n",
    "    temp.drop('RECORD', axis=1, inplace=True)\n",
    "    return(temp)\n",
    "\n",
    "# Special treatment, adding Yasmin's data from other dataloggers\n",
    "def add_other_tc_data(temp, temp_add):\n",
    "    # Now add the columns from the additional files, if they exist\n",
    "    merged = temp.merge(temp_add, on=['timestamp','dayid'], how='left')\n",
    "    return(merged)\n",
    "\n",
    "# Writes output files in full temporal resolution\n",
    "def write_output_file(out_df, date_idx, out_dir, output_fn):\n",
    "    out_df = out_df.copy()\n",
    "    # Drop duplicates\n",
    "    out_df.drop_duplicates(subset = 'timestamp', inplace=True)\n",
    "    # Sort, in case it's not yet the case\n",
    "    out_df.sort_values('timestamp', inplace=True)\n",
    "    \n",
    "    # Check if output folders exist. If not, create\n",
    "    month_dir = str(date_idx)[0:4] + \"-\" + str(date_idx)[4:6]\n",
    "    if(not os.path.exists(out_dir + month_dir)): # make directory if it doesn't exist\n",
    "        os.makedirs(out_dir + month_dir)\n",
    "    # Create file name\n",
    "    out_fn = out_dir + month_dir + \"/\" + output_fn + \"_\" + str(date_idx) + \".csv\"\n",
    "    #print(str(len(out_df.dayid)), out_fn) # Shows final file size)\n",
    "    # organise data for output\n",
    "    temp_df = out_df\n",
    "    # Before saving, remove the index\n",
    "    temp_df.drop('dayid', axis=1, inplace=True)\n",
    "    # Move timestamp column to the front\n",
    "    col = temp_df.pop('timestamp')\n",
    "    temp_df.insert(0, col.name, col, allow_duplicates=True)\n",
    "    # Remove timezone information\n",
    "    temp_df['timestamp'] = temp_df['timestamp'].dt.tz_localize(None)\n",
    "    # Write data\n",
    "    temp_df.to_csv(out_fn, index=False, encoding='utf-8', date_format='%Y-%m-%d %H:%M:%S') # Save file\n",
    "    \n",
    "# Creates a daily file for each type\n",
    "def organise_files(project_path, output_path, filetype, tc_add = False):\n",
    "    # List all files in the directories\n",
    "    fn_list = sorted(glob.glob(project_path + '*/*', recursive=True))\n",
    "    if(filetype == 'laser'):\n",
    "        fn_list = sorted(glob.glob(project_path + '*/*.str', recursive=True))\n",
    "    saved = []\n",
    "    \n",
    "    # Create output path name\n",
    "    project_path_output = output_path + project_path.split('/')[-2] + '/'\n",
    "\n",
    "    # For all files in the directory\n",
    "    for fn_i, fn in enumerate(fn_list):\n",
    "        # Only run data in the month list\n",
    "        current_month = fn.replace(project_path[:-1], \"\")[1:8]\n",
    "        if((current_month in month_list) or (len(month_list) == 0)):\n",
    "            if( (filetype == 'irga') & (int(current_month.replace('-','')) < 202007) ):\n",
    "                continue\n",
    "            #display(fn.replace(project_path_chambers[:-1], \"\")[1:8])\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Debugging message\n",
    "        if(fn_i % 1 == 0): # % 20 to show every 20th file being loaded\n",
    "            print( '{:<07}'.format(str(round(fn_i * 100 / len(fn_list), 4))) + \"%\\t\\t\" + fn.split('\\\\')[-2] + '/' + fn.split('\\\\')[-1]) # Show status\n",
    "            #print( '{:<07}'.format(str(round(fn_i * 100 / len(fn_list), 4))) + \"%\\t\\t\" + fn.replace(project_path + \"01_rawdata/\", \"\")) # Show status\n",
    "    \n",
    "        # Load the current laser file\n",
    "        if(filetype == 'laser'):\n",
    "            df = read_laser_file(fn)\n",
    "        else:\n",
    "            df = read_cr1000_file(fn)\n",
    "            if(filetype == 'irga'):\n",
    "                df = remove_obsolete_irga_data(df)\n",
    "                project_path_output = output_path + 'ambient/'\n",
    "            if(filetype == 'tc'):\n",
    "                df = fix_tc_names(df)\n",
    "    \n",
    "        if (fn_i != len(fn_list)-1):\n",
    "            # Load next file\n",
    "            if(filetype == 'laser'):\n",
    "                df_next = read_laser_file_by_lines(fn_list[fn_i+1], lines=1)\n",
    "            else:\n",
    "                df_next = read_cr1000_file_by_lines(fn_list[fn_i+1], lines=1)\n",
    "            next_day = df_next['dayid'].tolist()[0]\n",
    "            final_file = False\n",
    "        else:\n",
    "            next_day = 0\n",
    "            final_file = True\n",
    "  \n",
    "        # Group by per-day id\n",
    "        grouped = df.groupby(['dayid'])\n",
    "        #print(\"    Days in this file:  \", len(grouped))\n",
    "    \n",
    "        # For each group\n",
    "        for group_i, (this_day, day) in enumerate(grouped):\n",
    "        \n",
    "            # If we have saved data and if the day matches append the current\n",
    "            if (len(saved) > 0):\n",
    "                if (this_day == saved['dayid'].tolist()[0]):\n",
    "                    day = pd.concat([saved, day], axis=0, ignore_index=True)\n",
    "                    #print(\"    Current line count: \", str(len(day.dayid)))\n",
    "        \n",
    "            #If this is the final group in the file and this is not the final file and the first group of the next file is the same day\n",
    "            if (group_i == len(grouped)-1) and (not final_file) and (this_day == next_day):\n",
    "                saved = day\n",
    "                continue  \n",
    "            else:\n",
    "                if(tc_add): # Add special data from other dataloggers (from Yasmin Bohak)\n",
    "                    day = add_other_tc_data(day, tc_additional)\n",
    "                write_output_file(day, this_day, project_path_output, filetype)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a list of relevant dates and corresponding columns\n",
    "# (By Yasmin L Bohak)\n",
    "def make_list_from_tc_logbook(project_path,chambers_sensors_fn, silent=True):\n",
    "    import datetime\n",
    "    if(not silent): print('Creating list of relevant dates:')\n",
    "    #Read logbook tc to make list of lists of [sensor_name, start, end] dates\n",
    "    dates_thermocouples_1 = []\n",
    "    dates_thermocouples_2 = []\n",
    "\n",
    "    tc_log = pd.read_excel(project_path + chambers_sensors_fn, sheet_name='tc') #_additional + chambers_sensors_fn, sheet_name='tc')\n",
    "    tc_log.drop(['comment'], axis=1, inplace=True)\n",
    "    tc_log.dropna(subset=['date'], inplace=True)\n",
    "    tc_log['date']=pd.to_datetime(tc_log['date'].astype(str), format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    done = []\n",
    "    cols = tc_log.columns\n",
    "    if(not silent): print('  ',cols)\n",
    "    for col in cols:\n",
    "        if col != 'date':\n",
    "            tc_log[col]=pd.to_numeric(tc_log[col])\n",
    "\n",
    "    for i, row in tc_log.iterrows():\n",
    "        for c in range(1,len(cols)):\n",
    "            if c in done:\n",
    "            # If this condition is fulfilled then it means that this column was already\n",
    "            # fully parsed \n",
    "                continue\n",
    "            # Make lists of date ranges that will need to select from each data source\n",
    "            if row[c] >= 10:\n",
    "                mux = row[c]\n",
    "                # Iterate over rest of file to find end date, if doesn't exist, end date is today\n",
    "                end_index = i + 1\n",
    "                append_check = False\n",
    "                while end_index < tc_log.shape[0]:\n",
    "                    if(not silent): print('  - End index',end_index)\n",
    "                    if tc_log.iloc[end_index, c] == mux:\n",
    "                        end_index += 1\n",
    "                    else:\n",
    "                        if(not silent): print('  - Entry added',mux, row[0], tc_log.iloc[end_index, 0])\n",
    "                        # Add data to correct list (logger 1 or 2)\n",
    "                        if row[c] < 20:\n",
    "                            dates_thermocouples_1.append([cols[c], mux, row[0], tc_log.iloc[end_index, 0]])\n",
    "                        if row[c] >= 20:\n",
    "                            dates_thermocouples_2.append([cols[c], mux, row[0], tc_log.iloc[end_index, 0]])\n",
    "                        append_check = True\n",
    "                        break\n",
    "                if not append_check:\n",
    "                    today_ts = datetime.datetime.now()\n",
    "                    today_ts_rounded = today_ts - pd.Timedelta(microseconds=today_ts.microsecond)\n",
    "                    today_ts_rounded = pd.to_datetime(today_ts_rounded, format='%Y-%m-%d %H:%M:%S')\n",
    "                    #add data to correct list (logger 1 or 2)\n",
    "                    if row[c] < 20:\n",
    "                        dates_thermocouples_1.append([cols[c], mux, row[0], today_ts_rounded])\n",
    "                        done.append(c)\n",
    "                    if row[c] >= 20:\n",
    "                        dates_thermocouples_2.append([cols[c], mux, row[0], today_ts_rounded])\n",
    "                        done.append(c)\n",
    "    if(not silent): print('    Output:', dates_thermocouples_1, dates_thermocouples_2)\n",
    "    return(dates_thermocouples_1, dates_thermocouples_2)\n",
    "\n",
    "# (converts the mux data from list to mux name in the raw data)\n",
    "# this function works on the entire combined data set (output of function entire_data_set)\n",
    "# (By Yasmin L Bohak)\n",
    "def filter_mux_dates(df_n_filtered, dates_thermocouples):\n",
    "    data_set_2 = False\n",
    "    muxes = []\n",
    "    # Filter out all mux channels that are not related to the project\n",
    "    for listy in dates_thermocouples:\n",
    "        if listy[1] >= 20:\n",
    "            # When 20+ values, then assume soil data as mux might exist also for non-soil data\n",
    "            data_set_2 = True\n",
    "            mux = int(listy[1] - 20)\n",
    "            if mux not in muxes: \n",
    "                muxes.append(mux)\n",
    "        else:\n",
    "            mux = int(listy[1] - 10)\n",
    "            if mux not in muxes: \n",
    "                muxes.append(mux)\n",
    "    # Filter by columns but don't delete date and dayid (based on mux channels)\n",
    "    muxes_to_filter = []\n",
    "    muxes_renamed = []\n",
    "    if not data_set_2: # Meaning that we are dealing with data set 1\n",
    "        for mux in muxes:\n",
    "            muxes_to_filter.append('T' + str(mux))\n",
    "            muxes_renamed.append('Tc(' + str(10+mux) + ')')\n",
    "    else:\n",
    "        for mux in muxes:\n",
    "            muxes_to_filter.append('Tc_soil_Avg('+str(mux)+')')\n",
    "            muxes_renamed.append('Tc(' + str(20+mux) + ')')\n",
    "    muxes_to_filter.append('timestamp')\n",
    "    muxes_to_filter.append('dayid')\n",
    "    muxes_renamed.append('timestamp')\n",
    "    muxes_renamed.append('dayid')\n",
    "    df_filtered = df_n_filtered[muxes_to_filter]\n",
    "    # Rename columns\n",
    "    df_filtered.columns = muxes_renamed\n",
    "    \n",
    "    return(df_filtered)\n",
    "    \n",
    "# This function reads all the files in each data set,\n",
    "# connects them, filters out duplicate data, unifies the timestamp data format and\n",
    "# adds the dayid field\n",
    "# (By Yasmin L Bohak)\n",
    "def load_entire_data_set(project_path_tc_add, silent=False):\n",
    "    # Note: If using this function in order to read data from an excel file (not raw .dat file),\n",
    "    #       the function will assume that the first row of data contains the title and all the rest\n",
    "    #       of the file contains only data. (not related to files with .csv or .xlsx)\n",
    "    # For .csv files, they need to have a date-time format: 2022-05-04 hh:mm:ss\n",
    "    #       If this does not work save file as .xlsx file and then will work regardless of date time format.\n",
    "    if(not silent): print('Loading all data in', project_path_tc_add)\n",
    "    fn_list = sorted(glob.glob(project_path_tc_add + '*/*', recursive=True))\n",
    "    # df for entire additional data set of tcs\n",
    "    df_data_set = pd.DataFrame()\n",
    "    # Iterate over the files in the list\n",
    "    for fn_i, fn in enumerate(fn_list): \n",
    "        if(not silent): # Show status\n",
    "            print( '    {:<07}'.format(str(round(fn_i * 100 / len(fn_list), 4))) + \"%\\t\\t\" + fn.split('\\\\')[-2] + '/' + fn.split('\\\\')[-1])\n",
    "        extension = os.path.splitext(fn)[1]\n",
    "        if extension == '.dat':\n",
    "            df = read_cr1000_file(fn)        \n",
    "        elif extension == '.csv':\n",
    "            df = pd.read_csv(fn, na_values=[\"NAN\"])\n",
    "        elif extension == '.xlsx':\n",
    "            df = pd.read_excel(fn, na_values=[\"NAN\"])\n",
    "        # Add dayid\n",
    "        df.rename(columns={'TIMESTAMP':'timestamp'}, inplace=True)\n",
    "        df['timestamp'] = pd.to_datetime(df.timestamp, format='%Y-%m-%d %H:%M:%S', utc=True, errors=\"raise\")#errors='coerce')\n",
    "        #df['timestamp'] = df['timestamp'].dt.tz_localize(None)\n",
    "        df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "        df_data_set = pd.concat([df_data_set, df],axis=0)\n",
    "    before = df_data_set.shape\n",
    "    # Remove duplicates from final df (same row of data might exist in two files)\n",
    "    df_data_set.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "    after = df_data_set.shape\n",
    "    removed = before[0] - after[0]\n",
    "    if removed != 0:\n",
    "        print('    ', removed,'lines of duplicate data were removed from data set:', project_path_tc_add.split(\"/\")[-2])\n",
    "    return(df_data_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load additional data from additional Tc dataloggers (Yasmin L Bohak)\n",
    "\n",
    "# Additional data, logbook etc.\n",
    "project_path_additional = project_path + '00_additional_data/'\n",
    "chambers_sensors_fn = 'COS_chambers_sensor_logbook.xlsx'\n",
    "\n",
    "#paths to the additional Tc data, not in main tc data set\n",
    "project_path_tc1 = project_path + '01_rawdata/thermocouples_1/'\n",
    "project_path_tc2 = project_path + '01_rawdata/thermocouples_2/'\n",
    "\n",
    "# Extract relevant dates\n",
    "dates_tc1, dates_tc2 = make_list_from_tc_logbook(project_path_additional, chambers_sensors_fn, silent=True)\n",
    "\n",
    "# Load all data from 2 additional dataloggers\n",
    "tc1_df = load_entire_data_set(project_path_tc1)\n",
    "tc2_df = load_entire_data_set(project_path_tc2)\n",
    "\n",
    "# Filter relevant dates and rename columns\n",
    "out_tc1 = filter_mux_dates(tc1_df, dates_tc1)\n",
    "out_tc2 = filter_mux_dates(tc2_df, dates_tc2)\n",
    "\n",
    "# Merge\n",
    "tc_additional = out_tc1.merge(out_tc2, on=['timestamp','dayid'], how='outer')\n",
    "\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert files to daily output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run calculations\n",
    "#-----------------\n",
    "\n",
    "print('Laser files')\n",
    "organise_files(project_path_laser, project_path_output, 'laser')\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PAR files')\n",
    "organise_files(project_path_par, project_path_output, 'par')\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Thermocouple files')\n",
    "#organise_files(project_path_tc, project_path_output, 'tc') # Normal case\n",
    "organise_files(project_path_tc, project_path_output, 'tc', tc_add = True)\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Flow rate files')\n",
    "organise_files(project_path_flow, project_path_output, 'flow')\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRGA files needed for the ambient data. There is a lot, so it's slow...\n",
    "print('IRGA files')\n",
    "organise_files(project_path_irga, project_path_output, 'irga')\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data replacement\n",
    "\n",
    "There were gaps in the measurements due to device failure. Data was considered to be stable during this time period and was therefore replaced as follows:\n",
    "\n",
    "- FR gap:    [started at 11th July 2021 end 7th Aug 2021], we will use 10th July 2021 for July days, and 9th Aug 2021 for Aug days  \n",
    "- Tc gap:    [started at 13th July 2021 end 23rd July 2021], we will use 11th July 2021 for July days\n",
    "- PAR gap:   [started at 11th July 2021 end 7th Aug 2021], we will use 10th July 2021 for July days, and 8th Aug 2021 for Aug days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace bad Tc, July 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'thermocouples/' + '2021-07/tc_20210711.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(14, 23):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'thermocouples/', 'tc')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace bad flow, July 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'flow/' + '2021-07/flow_20210710.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(11, 32):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'flow/', 'flow')\n",
    "    pass\n",
    "\n",
    "# Replace bad flow, Aug 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'flow/' + '2021-08/flow_20210809.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(1, 8):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'flow/', 'flow')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace bad PAR, July 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'PAR/' + '2021-07/par_20210710.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(12, 32):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'PAR/', 'par')\n",
    "    pass\n",
    "\n",
    "# Replace bad PAR, Aug 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'PAR/' + '2021-08/par_20210808.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(1, 8):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    #display(out_df)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'PAR/', 'par')\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
